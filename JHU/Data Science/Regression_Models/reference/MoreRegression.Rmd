---
title: "MoreRegression"
output: html_document
---

See http://data.princeton.edu/R/linearModels.html

#Linear Models

Let us try some linear models, starting with multiple regression and analysis of covariance models, and then moving on to models using regression splines. In this section I will use the data read in Section 3, so make sure the fpe data frame is attached to your current session. 

##Fitting a Model

To fit an ordinary linear model with fertility change as the response and setting and effort as predictors, try 
```{r}
fpe <- read.table("http://data.princeton.edu/wws509/datasets/effort.dat")
lmfit = with(fpe,lm(change ~ setting + effort ))
```

Note first that lm is a function, and we assign the result to an object that we choose to call lmfit (for linear model fit). This stores the results of the fit for later examination. 

The argument to lm is a model formula, which has the response on the left of the tilde ~ (read "is modeled as") and a Wilkinson-Rogers model specification formula on the right. 

A nice feature of R is that it lets you create interactions between categorical variables, between categorical and continuous variables, and even between numeric variables (it just creates the cross-product). 

##Examining a Fit

Let us look at the results of the fit. One thing you can do with lmfit, as you can with any R object, is print it. 
```{r}
lmfit
```

The output includes the model formula and the coefficients. You can get a bit more detail by requesting a summary: 
```{r} 
summary(lmfit)
```
 
The output includes a more conventional table with parameter estimates and standard errors, as well the residual standard error and multiple R-squared. (By default S-Plus includes the matrix of correlations among parameter estimates, which is often bulky, while R sensibly omits it. If you really need it, add the option correlation=TRUE to the call to summary.) 

To get a hierarchical analysis of variance table corresponding to introducing each of the terms in the model one at a time, in the same order as in the model formula, try the anova function: 
```{r}
anova(lmfit)
```

Alternatively, you can plot the results using 
```{r}
par(mfrow=c(2,2))
plot(lmfit)
```

This will produce a set of four plots: residuals versus fitted values, a Q-Q plot of standardized residuals, a scale-location plot (square roots of standardized residuals versus fitted values, and a plot of residuals versus leverage that adds bands corresponding to Cook's distances of 0.5 and 1. 

R will prompt you to click on the graph window or press Enter before showing each plot, but we can do better. Type par(mfrow=c(2,2)) to set your graphics window to show four plots at once, in a layout with 2 rows and 2 columns. Then redo the graph using plot(lmfit). To go back to a single graph per window use par(mfrow=c(1,1)). There are many other ways to customize your graphs by setting high-level parameters, type ?par to learn more. 

Technical Note: You may have noticed that we have used the function plot with all kinds of arguments: one or two variables, a data frame, and now a linear model fit. In R jargon plot is a generic function. It checks for the kind of object that you are plotting and then calls the appropriate (more specialized) function to do the work. There are actually many plot functions in R, including plot.data.frame and plot.lm. For most purposes the generic function will do the right thing and you don't need to be concerned about its inner workings. 

##Extracting Results

There are some specialized functions that allow you to extract elements from a linear model fit. For example 
```{r}
fitted(lmfit)
```

extracts the fitted values. In this case it will also print them, because we did not asign them to anything. (The longer form fitted.values is an alias.) 

To extract the coefficients use the coef function (or the longer form coefficients) 
```{r}
coef(lmfit)
``` 
To get the residuals, use the residuals function (or the abbreviation resid): 
```{r}
residuals(lmfit)
```

If you are curious to see exactly what a linear model fit produces, try the function 
```{r}
names(lmfit)
```
which lists the named components of a linear fit. All of these objects may be extracted using the $ operator. However, whenever there is a special extractor function you are encouraged to use it. 
 
##Factors and Covariates

So far our predictors have been continuous variables or covariates. We can also use categorical variables or factors. Let us group family planning effort into three categories: 
```{r}
effortg = cut(fpe$effort, breaks = c(-1, 4, 14, 100), label=c("weak","moderate","strong"))
```
The function cut creates a factor or categorical variable. The first argument is an input vector, the second is a vector of breakpoints, and the third is a vector of category labels. Note that there is one more breakpoint than there are categories. All values greater than the i-th breakpoint and less than or equal to the (i+1)-st breakpoint go into the i-th category. Any values below the first breakpoint or above the last one are coded NA (a special R code for missing values). If the labels are omitted, R generates a suitable default of the form "(a,b]". By default the intervals are closed on the right, so our intervals are < 4; 5-14; 15+. To change this use the option right=FALSE.

Try fitting the analysis of covariance model: 
```{r}
covfit = lm(fpe$change ~ fpe$setting + effortg)
covfit
```
As you can see, family planning effort has been treated automatically as a factor, and R has generated the necessary dummy variables for moderate and strong programs treating weak as the reference cell. 

Choice of Contrasts: R codes unordered factors using the reference cell or "treatment contrast" method. The reference cell is always the first category which, depending on how the factor was created, is usually the first in alphabetical order. If you don't like this choice, R provides a special function to re-order levels, check out help(relevel). 

S codes unordered factors using the  Helmert contrasts by default, a choice that is useful in designed experiments because it produces orthogonal comparisons, but has baffled many a new user. Both R and S-Plus code ordered factors using polynomials. To change to the reference cell method for unordered factors use the following call 
```{r}
options(contrasts=c("contr.treatment","contr.poly"))
```

Back on to our analysis of covariance fit. You can obtain a hierarchical anova table for the analysis of covariance model using the anova function: 
```{r}
anova(covfit)
```

##Regression Splines

The real power of R begins to shine when you consider some of the other functions you can include in a model formula. First, you can include mathematical functions, for example log(setting) is a perfectly legal term in a model formula. You don't have to create a variable representing the log of setting and then use it, R will create it 'on the fly', so you can type 
```{r}
lm(fpe$change ~ log(fpe$setting) + fpe$effort)
```

If you wanted to use orthogonal polynomials of degree 3 on setting, you could include a term of the form poly(setting,3) 

You can also get R to calculate a well-conditioned basis for regression splines. First you must load the splines library (this step is not needed in S-Plus):
```{r message=FALSE}
library(splines)
```

This makes available the function bs to generate B-splines. For example the call 
```{r}
setting.bs <- bs(fpe$setting, knots = c(66,74,84) + fpe$effort)
```

will generate cubic B-splines with interior knots placed at 66, 74 and 84. This basis will use seven degrees of freedom, four corresponding to the constant, linear, quadratic and cubic terms, plus one for each interior knot. Alternatively, you may specify the number of degrees of freedom you are willing to spend on the fit using the parameter df. For cubic splines R will choose df-4 interior knots placed at suitable quantiles. You can also control the degree of the spline using the parameter degree, the default being cubic. 

If you like natural cubic splines, you can obtain a well-conditioned basis using the function ns, which has exactly the same arguments as bs except for degree, which is always three. To fit a natural spline with five degrees of freedom, use the call 
```{r}
setting.ns <- ns(fpe$setting, df=5)
```

Natural cubic splines are better behaved than ordinary splines at the extremes of the range. The restrictions mean that you save four degrees of freedom. You will probably want to use two of them to place additional knots at the extremes, but you can still save the other two. 

To fit an additive model to fertility change using natural cubic splines on setting and effort with only one interior knot each, placed exactly at the median of each variable, try the following call: 
```{r}
splinefit = lm(fpe$change ~ ns(fpe$setting, knot=median(fpe$setting)) + 
    ns(fpe$effort, knot=median(fpe$effort)) )
```

Here we used the parameter knot to specify where we wanted the knot placed, and the function median to calculate the median of setting and effort. 

Do you think the linear model was a good fit? Natural cubic splines with exactly one interior knot require the same number of parameters as an ordinary cubic polynomial, but are much better behaved at the extremes. 

##Other Options

The lm function has several additional parameters that we have not discussed. These include 

* data to specify a dataset, in case it is not attached 
* subset to restrict the analysis to a subset of the data 
* weights to do weighted least squares 

and many others; see help(lm) for further details. The args function lists the arguments used by any function, in case you forget them. Try args(lm). 

The fact that R has powerful matrix manipulation routines means that one can do many of these calculations from first principles. The next couple of lines create a model matrix to represent the constant, setting and effort, and then calculate the OLS estimate of the coefficients as (X'X)-1X'y: 
```{r}
X <- cbind(1,fpe$effort,fpe$setting)
solve( t(X) %*% X ) %*% t(X) %*% fpe$change
```      

Compare these results with coef(lmfit). 

#Generalized Linear Models

Generalized linear models are just as easy to fit in R as ordinary linear model. In fact, they require only an additional parameter to specify the variance and link functions. 

##Variance and Link Families

The basic tool for fitting generalized linear models is the glm function, which has the folllowing general structure: 

glm(formula, family, data, weights, subset, ...)

where ... stands for more esoteric options. The only parameter that we have not encountered before is family, which is a simple way of specifying a choice of variance and link functions. There are six choices of family: 

Family  |  Variance  |  Link
------  |  --------  |  ----
gaussian  |  gaussian  |  identity
binomial  |  binomial logit  |  probit or cloglog
poisson  | poisson  | log, identity or sqrt
Gamma  |  Gamma  | inverse, identity or log
inverse.gaussian  |  inverse.gaussian  |  1/mn^2
quasi  |  user-defined  |  user-defined

As can be seen, each of the first five choices has an associated variance function (for binomial the binomial variance m(1-m)), and one or more choices of link functions (for binomial the logit, probit or complementary log-log). 

As long as you want the default link, all you have to specify is the family name. If you want an alternative link, you must add a link argument. For example to do probits you use 
> glm( formula, family=binomial(link=probit))

The last family on the list, quasi, is there to allow fitting user-defined models by maximum quasi-likelihood. 

##Logistic Regression

We will illustrate fitting logistic regression models using the contraceptive use data shown below: 

   age education wantsMore notUsing using 
   <25       low       yes       53     6
   <25       low        no       10     4
   <25      high       yes      212    52
   <25      high        no       50    10
 25-29       low       yes       60    14
 25-29       low        no       19    10
 25-29      high       yes      155    54
 25-29      high        no       65    27
 30-39       low       yes      112    33
 30-39       low        no       77    80
 30-39      high       yes      118    46
 30-39      high        no       68    78
 40-49       low       yes       35     6
 40-49       low        no       46    48
 40-49      high       yes        8     8
 40-49      high        no       12    31

The data are available from the datasets section of the website for my generalized linear models course. Visit  http://data.princeton.edu/wws509/datasets to read a short description and follow the link to  cuse.dat. 

Of course the data can be downloaded directly from R: 
```{r}
cuse <- read.table("http://data.princeton.edu/wws509/datasets/cuse.dat", header=TRUE)
head(cuse)
```

I specified the header parameter as TRUE, because otherwise it would not have been obvious that the first line in the file has the variable names. There are no row names specified, so the rows will be numbered from 1 to 16. Print cuse to make sure you got the data in alright. Then make it your default dataset: 
```{r}
attach(cuse)
```

Let us first try a simple additive model where contraceptive use depends on age, education and wantsMore: 
```{r}
lrfit <- glm( cbind(using, notUsing) ~ age + education + wantsMore , family = binomial)
```

There are a few things to explain here. First, the function is called glm and I have assigned its value to an object called lrfit (for logistic regression fit). The first argument of the function is a model formula, which defines the response and linear predictor. 

With binomial data the response can be either a vector or a matrix with two columns. 

*  If the response is a vector it can be numeric with 0 for failure and 1 for success, or a factor with the first level representing "failure" and all others representing "success". In these cases R generates a vector of ones to represent the binomial denominators. 
*  Alternatively, the response can be a matrix where the first column is the number of "successes" and the second column is the number of "failures". In this case R adds the two columns together to produce the correct binomial denominator. 


Because the latter approach is clearly the right one for us I used the function cbind to create a matrix by binding the column vectors containing the numbers using and not using contraception. 

Following the special symbol ~ that separates the response from the predictors, we have a standard Wilkinson-Rogers model formula. In this case we are specifying main effects of age, education and wantsMore. Because all three predictors are categorical variables, they are treated automatically as factors, as you can see by inspecting the results:
```{r}
lrfit
```
Recall that R sorts the levels of a factor in alphabetical order. Because <25 comes before 25-29, 30-39, and 40-49, it has been picked as the reference cell for age. Similarly, high is the reference cell for education because high comes before low! Finally, R picked no as the base for wantsMore. 

If you are unhappy about these choices you can (1) use relevel to change the base category, or (2) define your own indicator variables. I will use the latter approach by defining indicators for women with high education and women who want no more children: 
```{r}
noMore <- wantsMore == "no"
hiEduc <- education == "high"
```

Now try the model again: 
```{r}
glm( cbind(using,notUsing) ~ age + hiEduc + noMore, family=binomial)
```

The residual deviance of 29.92 on 10 d.f. is highly significant: 
```{r}
1-pchisq(29.92,10)
```

so we need a better model. One of my favorites introduces an interaction between age and desire for no more children: 
```{r}
lrfit <- glm( cbind(using,notUsing) ~ age * noMore + hiEduc , family=binomial)
lrfit
```

Note how R built the interaction terms automatically, and even came up with sensible labels for them. The model's deviance of 12.63 on 7 d.f. is not significant at the conventional five per cent level, so we have no evidence against this model. 

To obtain more detailed information about this fit try the summary function: 
```{r}
summary(lrfit)
```

R follows the popular custom of flagging significant coefficients with one, two or three stars depending on their p-values. Try plot(lrfit). You get the same plots as in a linear model, but adapted to a generalized linear model; for example the residuals plotted are deviance residuals (the square root of the contribution of an observation to the deviance, with the same sign as the raw residual). 

The functions that can be used to extract results from the fit include 
*  residuals or resid, for the deviance residuals 
*  fitted or fitted.values, for the fitted values (estimated probabilities) 
*  predict, for the linear predictor (estimated logits) 
*  coef or coefficients, for the coefficients, and 
*  deviance, for the deviance. 

Some of these functions have optional arguments; for example, you can extract five different types of residuals, called "deviance", "pearson", "response" (response - fitted value), "working" (the working dependent variable in the IRLS algorithm - linear predictor), and "partial" (a matrix of working residuals formed by omitting each term in the model). You specify the one you want using the type argument, for example residuals(lrfit,type="pearson"). 

##Updating Models

If you want to modify a model you may consider using the special function update. For example to drop the age:noMore interaction in our model one could use 
```{r}
lrfit0 <- update(lrfit, ~ . - age:noMore)
```

The first argument is the result of a fit, and the second an updating formula. The place holder ~ separates the response from the predictors and the dot . refers to the right hand side of the original formula, so here we simply remove age:noMore. Alternatively, one can give a new formula as the second argument. 

The update function can be used to fit the same model to different datasets, using the argument data to specify a new data frame. Another useful argument is subset, to fit the model to a different subsample. This function works with linear models as well as generalized linear models. 

If you plan to fit a sequence of models you will find the anova function useful. Given a series of nested models, it will calculate the change in deviance between them. Try
```{r}
anova(lrfit0,lrfit)
```

Adding the interaction has reduced the deviance by 17.288 at the expense of 3 d.f. 

If the argument to anova is a single model, the function will show the change in deviance obtained by adding each of the terms in the order listed in the model formula, just as it did for linear models. Because this requires fitting as many models as there are terms in the formula, the function may take a while to complete its calculations. 

The anova function lets you specify an optional test. The usual choices will be "F" for linear models and "Chisq" for generalized linear models. Adding the parameter test="Chisq" adds p-values next to the deviances. In our case 
```{r}
anova(lrfit,test="Chisq")
``` 

We can see that all terms were highly significant when they were introduced into the model. 

##odel Selection

A very powerful tool in R is a function for stepwise regression that has three remarkable features: 

1.  It works with generalized linear models, so it will do stepwise logistic regression, or stepwise Poisson regression,
2.  It understand about hierarchical models, so it will only consider adding interactions only after including the corresponding main effects in the models, and
3.  It understands terms involving more than one degree of freedom, so it it will keep together dummy variables representing the effects of a factor.

The basic idea of the procedure is to start from a given model (which could well be the null model) and take a series of steps by either deleting a term already in the model or adding a term from a list of candidates for inclusion, called the scope of the search and defined, of course, by a model formula. 

Selection of terms for deletion or inclusion is based on Akaike's information criterion (AIC). R defines AIC as 

â€“2 maximized log-likelihood + 2 number of parameters

(S-Plus defines it as the deviance minus twice the number of parameters in the model. The two definitions differ by a constant, so differences in AIC are the same in the two environments.) The procedure stops when the AIC criterion cannot be improved. 

In R all of this work is done by calling a couple of functions, add1 and drop1, that consider adding or dropping a term from a model. These functions can be very useful in model selection, and both of them accept a test argument just like anova. 

Consider first drop1. For our logistic regression model, 
```{r}
drop1(lrfit, test="Chisq")
```
Obviously we can't drop any of these terms. Note that R considered dropping the main effect of education and the age by want no more interaction, but did not examine the main effects of age or want no more, because one would not drop these main effects while retaining the interaction. 

The sister function add1 requires a scope to define the additional terms to be considered. In our example we will consider all possible two-factor interactions: 
```{r}
add1(lrfit, ~.^2,test="Chisq")
```
We see that neither of the missing two-factor interactions is significant by itself at the conventional five percent level. (However, they happen to be jointly significant.) Note that the model with the age by education interaction has a lower AIC than our starting model. 

The step function will do an automatic search. Here we let it search in a scope defined by all two-factor interactions: 
```{r}
#search <- step(additive, ~.^2)
```

The step function produces detailed trace output that we have supressed. The returned object, however, includes an anova component that summarizes the search: 
```{r}
search$anova
```

As you can see, the automated procedure introduced, one by one, all three remaining two-factor interactions, to yield a final AIC of 99.9. This is an example where AIC, by requiring a deviance improvement of only 2 per parameter, may have led to overfitting the data. 

Some analysts prefer a higher penalty per parameter. In particular, using log(n) instead of 2 as a multiplier yields BIC, the Bayesian Information Criterion. In our example log(1607) = 7.38, so we would require a deviance reduction of 7.38 per additional parameter. The step function accepts k as an argument, with default 2. You may verify that specifying k=log(1607) leads to a much simpler model; not only are no new interactions introduced, but the main effect of education is dropped (even though it is significant). 

